# Jarvis ‚Äî Assistant IA personnel

Assistant IA local avec RAG, m√©moire conversationnelle, reconnaissance vocale et synth√®se vocale. Tourne enti√®rement en local (aucune donn√©e envoy√©e dans le cloud).

## Sommaire

- [Architecture](#architecture)
- [Pr√©requis](#pr√©requis)
- [Installation](#installation)
- [D√©marrage](#d√©marrage)
- [Fonctionnalit√©s](#fonctionnalit√©s)
- [API Reference](#api-reference)
- [Variables d'environnement](#variables-denvironnement)
- [Feuille de route](#feuille-de-route)

---

## Architecture

```text
jarvis-full-solution/
‚îú‚îÄ‚îÄ jarvis/          # Backend API (NestJS, port 3000)
‚îú‚îÄ‚îÄ jarvis-ui/       # Frontend (Angular 21, port 4200)
‚îú‚îÄ‚îÄ stt-server/      # Transcription vocale (FastAPI/Whisper, port 8300)
‚îú‚îÄ‚îÄ wake-listener/   # D√©tection wake word "Hey Jarvis" (Python)
‚îî‚îÄ‚îÄ docker-qdrant/   # Base vectorielle Qdrant (Docker, port 6333)
```

### Flux de communication

```text
Angular (4200) ‚îÄ‚îÄHTTP/SSE‚îÄ‚îÄ‚ñ∫ NestJS (3000) ‚îÄ‚îÄ‚ñ∫ Ollama (11434)    [LLM + Embeddings]
                                            ‚îÄ‚îÄ‚ñ∫ Qdrant (6333)    [Recherche vectorielle]
                                            ‚îÄ‚îÄ‚ñ∫ STT Server (8300) [Transcription audio]

Wake Listener (micro) ‚îÄ‚îÄ‚ñ∫ STT Server (8300) [Transcription]
                      ‚îÄ‚îÄ‚ñ∫ NestJS (3000)     [/memory/add, /memory/query]
```

### Flux principaux

#### Ingestion document

```text
Upload (PDF/TXT/MD) ‚Üí LangChain parsing ‚Üí chunking (1000 chars, overlap 150)
  ‚Üí embeddings Ollama ‚Üí Qdrant (domainknowledge)
```

#### RAG Q&A (streaming)

```text
Question ‚Üí embedding ‚Üí recherche Qdrant top-5 ‚Üí contexte + prompt
  ‚Üí LLM gpt-oss:20b (stream SSE) ‚Üí r√©ponse
```

#### Wake Word ‚Üí M√©moire

```text
Microphone ‚Üí "Hey Jarvis" (OpenWakeWord) ‚Üí enregistrement silence RMS ‚Üí WAV
  ‚Üí STT Whisper ‚Üí CommandClassifier (ADD / QUERY / UNKNOWN)
      ADD   ‚Üí /memory/add  ‚Üí TemporalService ‚Üí Qdrant (jarvis_for_home) ‚Üí TTS "C'est not√©."
      QUERY ‚Üí /memory/query ‚Üí TemporalService ‚Üí Qdrant search ‚Üí LLM ‚Üí TTS r√©ponse
```

---

## Pr√©requis

| Outil | Version | Usage |
| ----- | ------- | ----- |
| Node.js | 20+ | Backend NestJS + Frontend Angular |
| Python | 3.10+ | STT server + Wake listener |
| Docker | - | Qdrant (base vectorielle) |
| [Ollama](https://ollama.com) | latest | LLM local + embeddings |

### Mod√®les Ollama requis

```bash
ollama pull qwen3-embedding:8b   # embeddings (4096 dims)
ollama pull qwen3:4b             # LLM small (classification, intention)
ollama pull mistral:latest       # LLM medium (m√©moire, r√©sum√©s)
ollama pull gpt-oss:20b          # LLM large (RAG, raisonnement)
```

> Les trois mod√®les LLM sont configurables via `.env`. Seul l'embed model est requis pour le RAG de base.

---

## Installation

### 1. Base vectorielle (Qdrant)

```bash
cd docker-qdrant
docker-compose up -d
```

Qdrant sera disponible sur `http://localhost:6333`.

### 2. Backend NestJS

```bash
cd jarvis
cp .env.example .env   # adapter les valeurs si besoin
npm install
```

### 3. Frontend Angular

```bash
cd jarvis-ui
npm install
```

### 4. STT Server

```bash
cd stt-server
pip install -r requirements.txt
cp .env.example .env
```

### 5. Wake Listener (optionnel)

```bash
cd wake-listener
pip install -r requirements.txt
cp .env.example .env
```

---

## D√©marrage

Lancer chaque composant dans un terminal s√©par√© :

```bash
# 1. Qdrant
cd docker-qdrant && docker-compose up -d

# 2. Backend
cd jarvis && npm run start:dev

# 3. Frontend
cd jarvis-ui && npm start

# 4. STT Server
cd stt-server && python stt_server.py

# 5. Wake Listener (optionnel)
cd wake-listener && python wake_listener.py
```

L'interface est disponible sur [http://localhost:4200](http://localhost:4200).
La doc Swagger du backend est sur [http://localhost:3000/api](http://localhost:3000/api).

---

## Fonctionnalit√©s

### RAG ‚Äî Base documentaire

- Ingestion de fichiers PDF, TXT et Markdown (jusqu'√† 50 MB)
- Chunking automatique (1000 caract√®res, overlap 150)
- Recherche s√©mantique avec top-K configurable
- R√©ponses stream√©es (Server-Sent Events)

### M√©moire conversationnelle

- Stockage de faits avec contexte temporel (`"ce soir √† 20h"`, `"vendredi prochain"`)
- Recherche s√©mantique avec filtre de plage de dates ou d'intervalle (ex. "la semaine derni√®re")
- D√©tection de r√©currence (`"tous les mardis"`) et de direction temporelle (pass√© / futur)
- Q&A en langage naturel avec r√©ponse LLM en fran√ßais
- Event bus interne (EventEmitter2) pour les op√©rations m√©moire

### Reconnaissance vocale

- Entr√©e microphone depuis l'interface web (WebM ‚Üí STT)
- Transcription via Whisper turbo (fran√ßais)
- Wake word "Hey Jarvis" pour utilisation mains-libres

### Synth√®se vocale

- R√©ponses vocales via Piper TTS local (mod√®le `fr_FR-siwis-medium`)
- T√©l√©chargement automatique du mod√®le au premier d√©marrage

### Multi-mod√®le Ollama

| Taille | Mod√®le | Usage |
| ------ | ------ | ----- |
| small | `qwen3:4b` | Classification, intention |
| medium | `mistral:latest` | M√©moire, r√©sum√©s |
| large | `gpt-oss:20b` | RAG, raisonnement complexe |

---

## API Reference

### RAG

| M√©thode | Route | Description |
| ------- | ----- | ----------- |
| `POST` | `/rag/ingest` | Ingestion fichier ou texte brut (max 50 MB) |
| `POST` | `/rag/ask` | Q&A avec contexte documentaire |
| `POST` | `/rag/ask/stream` | Q&A stream√© (SSE) ‚Äî √©met `event: metadata` puis tokens |

### M√©moire

| M√©thode | Route | Description |
| ------- | ----- | ----------- |
| `POST` | `/memory/add` | Stocker un fait avec contexte temporel optionnel |
| `POST` | `/memory/search` | Recherche s√©mantique avec filtre de dates |
| `POST` | `/memory/query` | Q&A complet en langage naturel |

### LLM direct

| M√©thode | Route | Description |
| ------- | ----- | ----------- |
| `POST` | `/llm/ask` | G√©n√©ration LLM sans contexte RAG |
| `POST` | `/llm/ask/stream` | G√©n√©ration stream√©e (SSE) |

### STT

| M√©thode | Route | Description |
| ------- | ----- | ----------- |
| `POST` | `/stt/transcribe` | Transcription audio (max 25 MB, proxy Whisper) |

### Agent (Phase 2.2+)

| M√©thode | Route | Description |
| ------- | ----- | ----------- |
| `POST` | `/agent/classify` | Classification d'intention LLM (endpoint activ√© en Phase 2.2) |

### Sant√©

| M√©thode | Route | Description |
| ------- | ----- | ----------- |
| `GET` | `/health` | Check de sant√© du backend |

---

## Variables d'environnement

### `jarvis/.env`

```env
# CORS
CORS_ORIGINS=http://localhost:4200

# Ollama
OLLAMA_BASE_URL=http://127.0.0.1:11434
OLLAMA_LLM_SMALL_MODEL=qwen3:4b
OLLAMA_LLM_MODEL=mistral:latest
OLLAMA_LLM_LARGE_MODEL=gpt-oss:20b
OLLAMA_EMBED_MODEL=qwen3-embedding:8b

# Qdrant
QDRANTURL=http://127.0.0.1:6333
QDRANTCOLLECTION=domainknowledge
QDRANT_MEMORY_COLLECTION=jarvis_for_home

# RAG
RAG_TOP_K=5
CHUNK_SIZE=1000
CHUNK_OVERLAP=150

# STT
STT_SERVER_URL=http://127.0.0.1:8300
```

### `stt-server/.env`

```env
STT_PORT=8300
WHISPER_MODEL=turbo
WHISPER_LANGUAGE=fr
WHISPER_DEVICE=auto
WHISPER_COMPUTE_TYPE=auto
```

### `wake-listener/.env`

```env
WAKE_MODEL=hey_jarvis
WAKE_THRESHOLD=0.5
SAMPLE_RATE=16000
SILENCE_THRESHOLD=500
SILENCE_DURATION_SEC=3.0
MAX_RECORDING_SEC=30.0
CHUNK_SIZE=1280
STT_SERVER_URL=http://127.0.0.1:8300
JARVIS_API_URL=http://127.0.0.1:3000
TTS_MODEL=fr_FR-siwis-medium
TTS_ENABLED=true
```

---

## Feuille de route

Le d√©veloppement suit un plan en 5 phases. Voir [plan/SUMMARY.md](plan/SUMMARY.md) pour le d√©tail.

| Phase | Titre | √âtat |
| ----- | ----- | ---- |
| **1** | Fondations ‚Äî types, event bus, multi-LLM, agent context, temporal enrichi | ‚úÖ 1.1‚Äì1.5 termin√©s |
| **2** | Intent engine & agent core ‚Äî classification LLM, routing | üöß 2.1 termin√©, 2.2‚Äì2.4 planifi√©s |
| **3** | M√©moire enrichie ‚Äî scoring, RAG hybride, knowledge graph | üîú planifi√© |
| **4** | Actions & proactivit√© ‚Äî action engine, goals, identity | üîú planifi√© |
| **5** | Profondeur cognitive ‚Äî context fusion, feedback, hallucination guard | üîú planifi√© |

### Phase 1 ‚Äî D√©tail des impl√©mentations

- **1.1** ‚Äî `MemoryPayload` s√©par√© de `RagPayload` (`src/memory/memory.types.ts`)
- **1.2** ‚Äî Event bus NestJS EventEmitter2 avec `MemoryEventsListener` (MEMORY_ADDED, MEMORY_SEARCHED, MEMORY_QUERIED)
- **1.3** ‚Äî Routage multi-mod√®le Ollama via `resolveModel('small'|'medium'|'large')`
- **1.4** ‚Äî Types `AgentContext`, `AgentResponse`, `ConversationMessage` dans `src/agent/agent.types.ts`
- **1.5** ‚Äî `TemporalService` enrichi : `parseInterval()` (plages de dates), `detectRecurrence()` (patterns r√©currents), `detectDirection()` (past/future) ; `MemoryService.query()` filtre automatiquement par intervalle `eventDate`

### Phase 2 ‚Äî D√©tail des impl√©mentations

- **2.1** ‚Äî `IntentEngine` (`src/agent/intent/`) : classification dual-path LLM (qwen3:4b) ‚Üí regex fallback ; `command_classifier.py` tente `POST /agent/classify` avant de tomber sur les regex

---

## Stack technique

| Composant | Technologies |
| --------- | ------------ |
| Backend | NestJS 11, TypeScript, LangChain, Qdrant JS Client, chrono-node |
| Frontend | Angular 21, Angular Material, vitest |
| STT | FastAPI, faster-whisper (Whisper turbo) |
| Wake listener | Python, OpenWakeWord, PyAudio, Piper TTS |
| LLM / Embeddings | Ollama (local) |
| Vector store | Qdrant (Docker) |
